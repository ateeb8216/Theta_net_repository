# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H5UQ--MvACc2umL7amm02W-ZrmnLL0tk
"""

"""
train.py
--------
Training pipeline for HLNN using UCI, BBC, and hybrid news datasets.
Includes early stopping, model checkpointing, and dataset placeholders.

Author: Academic HLNN Team
Python Version: 3.11
"""

import os
from preprocessing import load_dataset, encode_labels, tokenize_texts, hybrid_semantic_augmentation
from model_hlnn import build_hlnn
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical

# Dataset placeholders (update paths accordingly)
UCI_PATH = "data/uci_news.csv"
BBC_PATH = "data/bbc_news.csv"
HYBRID_PATH = "data/hybrid_dataset.csv"

# Load datasets
uci_df = load_dataset(UCI_PATH)
bbc_df = hybrid_semantic_augmentation(load_dataset(BBC_PATH))
hybrid_df = load_dataset(HYBRID_PATH)

# Combine datasets for training
train_df = pd.concat([uci_df, bbc_df])

# Encode labels
y, label_encoder = encode_labels(train_df['label'])
y = to_categorical(y, num_classes=9)

# Tokenize text
X, tokenizer = tokenize_texts(train_df['text'])
vocab_size = len(tokenizer.word_index)

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Build model
model, attention_weights = build_hlnn(vocab_size)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks
checkpoint = ModelCheckpoint("HLNN_best_model.h5", save_best_only=True, monitor='val_accuracy', mode='max')
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=16, callbacks=[checkpoint, early_stop])